{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Keywords and Classify Website with TF-IDF and GENSIM\n",
    "Input:\n",
    "```\n",
    "    URL\n",
    "```\n",
    "Output:\n",
    "```\n",
    "    Main output: Sport-site or Non-sport site. \n",
    "    Sub-output: vocabulary list.\n",
    "```\n",
    "File used:\n",
    "```\n",
    "link.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # work with html\n",
    "import requests \n",
    "\n",
    "import re #regular expression\n",
    "\n",
    "import string #to remove punctuation\n",
    "from nltk.corpus import stopwords #get stopwords\n",
    "from nltk.stem import WordNetLemmatizer #lemmatise words\n",
    "\n",
    "#tf-idf libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 following functions are to trim the scraped texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_to_string(url):\n",
    "    '''\n",
    "        Return the Tag-type data found to String-type. \n",
    "    '''\n",
    "    page = requests.get(url)\n",
    "    parsing = BeautifulSoup(page.text, \"html.parser\")\n",
    "    texts_tag = parsing.findAll('title') + parsing.findAll('p')\n",
    "    texts = str()\n",
    "    for text in texts_tag:\n",
    "        texts += str(text) + ' '\n",
    "    return texts\n",
    "\n",
    "def tag_removal(texts):\n",
    "    '''\n",
    "        Find all the tag of <.*>\n",
    "    '''\n",
    "    pattern = r\"<.*?>\"\n",
    "    findings = re.findall(pattern, texts)\n",
    "    return findings\n",
    "\n",
    "def get_texts(texts):\n",
    "    '''\n",
    "        Replace all the tag of <.*> to get texts\n",
    "    '''\n",
    "    findings = tag_removal(texts)\n",
    "    for finding in findings:\n",
    "        texts = texts.replace(finding, \"\")\n",
    "    return texts       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string #to remove punctuation\n",
    "from nltk.corpus import stopwords #get stopwords\n",
    "from nltk.stem import WordNetLemmatizer #lemmatise words\n",
    "\n",
    "def preprocess(text_data):\n",
    "    '''\n",
    "        Remove unwanted and preprocess data\n",
    "    '''\n",
    "    #lower text\n",
    "    text_data = text_data.lower()\n",
    "    \n",
    "    #remove punctuation\n",
    "    text_data = text_data.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    for stopword in stop_words:\n",
    "        pattern = ' ' + stopword + ' '\n",
    "        try:\n",
    "            text_data = text_data.replace(pattern, \" \")\n",
    "        except:\n",
    "            None\n",
    "    \n",
    "    \n",
    "    #remove numbers\n",
    "    text_data = re.sub(r\"[0-9]\", \"\", text_data)\n",
    "    \n",
    "    #remove big whitespace\n",
    "    text_data = re.sub(r\"\\s{2,}\", \" \", text_data)\n",
    "    \n",
    "    #remove non-latin character\n",
    "    text_data = re.sub(r\"[^a-zA-Z\\s]\", \"\", text_data)\n",
    "    \n",
    "    #lemmatize\n",
    "    text_data = text_data.split(' ')\n",
    "    lmt = WordNetLemmatizer() \n",
    "    text_data = list(map(lmt.lemmatize, text_data))\n",
    "    \n",
    "    \n",
    "    return ' '.join(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus(urls):\n",
    "    '''\n",
    "        Create a corpus for TF-IDF\n",
    "    '''\n",
    "    \n",
    "    docs = list()\n",
    "    \n",
    "    for url in urls:\n",
    "        texts = tag_to_string(url)\n",
    "        findings = tag_removal(texts)\n",
    "        texts = get_texts(texts)\n",
    "        docs.append(preprocess(texts))\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open links.txt file that contains links from BBC Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links.txt\", \"r\") as f:\n",
    "    links = f.readlines()\n",
    "\n",
    "docs = corpus(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vocabulary list by ignoring the one that appears in 80% of the text (`max_df=0.8`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove common words\n",
    "cv = CountVectorizer(max_df=0.8)\n",
    "\n",
    "#generate a td-matrix\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "\n",
    "#get feature names\n",
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the TF-IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_trans = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_trans.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to sort the the matrix and extract the number of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    '''\n",
    "        Sorting the data from coo matrix by descending order\n",
    "    '''\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def topn_from_vector(feature_names, sorted_items, topn=15):\n",
    "    '''\n",
    "        Extract the top n keywords\n",
    "    '''\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    \n",
    "    scores, features = list(), list()\n",
    "    \n",
    "    for i, score in sorted_items:\n",
    "        \n",
    "        scores.append(round(score, 3))\n",
    "        features.append(feature_names[i])\n",
    "        \n",
    "    results = dict()\n",
    "    for i in range(len(features)):\n",
    "        results[features[i]] = scores[i]\n",
    "        \n",
    "    return features, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to small sample size, let us compute the TF-IDF values and extract the keywords from each of the document from the corpus.\n",
    "\n",
    "This will be our set of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = set()\n",
    "for doc in docs:\n",
    "    #tf-idf for the given document\n",
    "    tfidf_vector = tfidf_trans.transform(cv.transform([doc]))\n",
    "    \n",
    "    #sorting by descending order\n",
    "    sorted_items = sort_coo(tfidf_vector.tocoo())\n",
    "    \n",
    "    #extract the keyword and its score\n",
    "    kw, scores = topn_from_vector(feature_names, sorted_items, 15)\n",
    "    \n",
    "    #generate a set of vocabularies\n",
    "    for k in kw:\n",
    "        terms.add(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with another website that is not in the links.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://www.bbc.com/sport/football/53788177\"\n",
    "texts = tag_to_string(url2)\n",
    "findings = tag_removal(texts)\n",
    "texts = get_texts(texts)\n",
    "texts = preprocess(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate keywords usign TF-IDF for the testing input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barcelona\n",
      "bayern\n",
      "world\n",
      "football\n"
     ]
    }
   ],
   "source": [
    "tfidf_vector = tfidf_trans.transform(cv.transform([texts]))\n",
    "sorted_items = sort_coo(tfidf_vector.tocoo())\n",
    "kw, scores = topn_from_vector(feature_names, sorted_items, 15)\n",
    "\n",
    "# see the matching keywords\n",
    "for k in kw:\n",
    "    if k in terms:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate keywords using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\USER\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "D:\\Users\\USER\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# gensim library to find out the keywords of a respective doc\n",
    "from gensim.summarization import keywords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('barcelona', 0.17417648493105373)\n",
      "('world', 0.1564135563343885)\n",
      "('game', 0.1542339617996264)\n",
      "('football', 0.13645411832181842)\n"
     ]
    }
   ],
   "source": [
    "rr = keywords(texts, words = 15, scores = True, lemmatize = True)\n",
    "\n",
    "# check the matching keywords\n",
    "for r in rr:\n",
    "    if r[0] in terms:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to small corpus, as long as >= 3 keywords from an article of choice match with the set of vocabularies, it can be induced that the website belongs to BBC Sport.\n",
    "\n",
    "Using TF-IDF technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sport page.\n"
     ]
    }
   ],
   "source": [
    "count = int()\n",
    "for k in kw:\n",
    "    if k in terms:\n",
    "        count += 1\n",
    "if count >= 3:\n",
    "    print('This is a Sport page.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
